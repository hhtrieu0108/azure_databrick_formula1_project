{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93610343-28e1-4421-9178-cf2b2817b95c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ingest circuits.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece2c25d-1538-4274-a6a4-adbd9cc83532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/configuration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73a6c68-4285-4728-a13a-01f1ee88a503",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 1 - Read the CSV file using the spark dataframe reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20e1184-437f-4e74-9fca-7d289c452b39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40ee56e-dea1-42bb-812d-ff28a3ce7bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_schema = StructType(fields=[\n",
    "    StructField(\"circuitId\", IntegerType(), nullable=False),\n",
    "    StructField(\"circuitRef\", StringType(), nullable=True),\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"location\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"lat\", DoubleType(), nullable=True),\n",
    "    StructField(\"lng\", DoubleType(), nullable=True),\n",
    "    StructField(\"alt\", IntegerType(), nullable=True),\n",
    "    StructField(\"url\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87405ae0-c7d3-49a1-bc92-a6b88462f15f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df_path = f\"{raw_folder_path}/circuits.csv\"\n",
    "\n",
    "circuits_df = spark.read.format(\"csv\")\\\n",
    "                        .option(\"header\", \"true\")\\\n",
    "                        .schema(schema=circuits_schema)\\\n",
    "                        .load(circuits_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e272de91-c616-456c-8a50-4daf25b0d54e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 2 - Select only required column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "52fae460-b2f8-4d4d-9889-5aa721205e81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# circuits_selected_df = circuits_df.select(\"circuitId\", \"circuitRef\", \"name\", \"location\", \"country\", \"lat\", \"lng\", \"alt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "97ac146d-3e7d-46fd-a7c3-1747017bd8a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# circuits_selected_df = circuits_df.select(circuits_df.circuitId, \n",
    "#                                           circuits_df.circuitRef, \n",
    "#                                           circuits_df.name, \n",
    "#                                           circuits_df.location, \n",
    "#                                           circuits_df.country,\n",
    "#                                           circuits_df.lat, \n",
    "#                                           circuits_df.lng, \n",
    "#                                           circuits_df.alt\n",
    "#                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ad9d3e11-1ddb-4776-843e-64985f0d10e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# circuits_selected_df = circuits_df.select(circuits_df[\"circuitId\"], \n",
    "#                                           circuits_df[\"circuitRef\"], \n",
    "#                                           circuits_df[\"name\"], \n",
    "#                                           circuits_df[\"location\"], \n",
    "#                                           circuits_df[\"country\"],\n",
    "#                                           circuits_df[\"lat\"], \n",
    "#                                           circuits_df[\"lng\"], \n",
    "#                                           circuits_df[\"alt\"]\n",
    "#                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a64b21-f25b-4841-84ae-1616e40b7d05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "circuits_selected_df = circuits_df.select(col(\"circuitId\"), \n",
    "                                          col(\"circuitRef\"), \n",
    "                                          col(\"name\"), \n",
    "                                          col(\"location\"), \n",
    "                                          col(\"country\").alias(\"race_country\"),\n",
    "                                          col(\"lat\"), \n",
    "                                          col(\"lng\"), \n",
    "                                          col(\"alt\")\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70013593-eaef-4849-98a3-9c5c157cdb92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 3 - Rename the columns as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc384da-43e3-4c54-915d-101fce686cb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_renamed_df = circuits_selected_df.withColumnRenamed(\"circuitId\", \"circuit_id\") \\\n",
    "                                          .withColumnRenamed(\"circuitRef\", \"circuit_ref\") \\\n",
    "                                          .withColumnRenamed(\"lat\", \"latitude\") \\\n",
    "                                          .withColumnRenamed(\"lng\", \"longitude\") \\\n",
    "                                          .withColumnRenamed(\"alt\", \"altitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285823c6-8dbd-469e-b9c8-3fac64d43b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 4 - Add ingestion date to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c049a004-4701-4348-a33f-af6924e38e14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "circuits_final_df = circuits_renamed_df.withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "                                       .withColumn(\"env\", lit(\"Production\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adae59f0-3e6a-42f9-be13-172a8aa93866",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 5 - Write data to datalake as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f51fb9-1de4-46fc-9eb6-b4a9d08ba01e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.circuits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb87fe43-07ea-43b1-8fd7-281a8d358e20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1145852851449767,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1.ingest_circuits_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
