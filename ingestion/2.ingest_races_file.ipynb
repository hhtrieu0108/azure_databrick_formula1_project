{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b8f95f9-c48d-4e59-b67c-e6b59f78b3c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ingest races.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348a9f9c-fde9-4ee0-b138-572f9a702b70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 1 -  Read the CSV file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41fac4d-68ac-4e8e-9f9e-8cf687ac2bb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59de6dea-578e-48d3-acf8-ea26887a5a69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_schema = StructType(fields=[\n",
    "    StructField(\"raceId\", IntegerType(), nullable = False),\n",
    "    StructField(\"year\", IntegerType(), nullable = True),\n",
    "    StructField(\"round\", IntegerType(), nullable = True),\n",
    "    StructField(\"circuitId\", IntegerType(), nullable = True),\n",
    "    StructField(\"name\", StringType(), nullable = True),\n",
    "    StructField(\"date\", DateType(), nullable = True),\n",
    "    StructField(\"time\", StringType(), nullable = True),\n",
    "    StructField(\"url\", StringType(), nullable = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174438d0-6bc9-4f02-98fe-ea8b1f8feda4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_df = spark.read.option(\"header\", \"True\")\\\n",
    "                     .schema(schema=races_schema)\\\n",
    "                     .format(\"csv\")\\\n",
    "                     .load(\"/mnt/tideformula1dl/raw/races.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0083b76-3db0-4595-8618-36f261e33829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 2 - Add ingestion date and race_timestamp to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62fd0a0-ba98-41dc-807f-c51131a193a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col, to_timestamp, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4092d7f6-2efd-4f8c-9c84-a2f01d37a851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_with_timestamp_df = races_df.withColumn(\"ingestion_date\", current_timestamp())\\\n",
    "                                  .withColumn(\"race_timestamp\", to_timestamp(\n",
    "                                                                            concat(\n",
    "                                                                                col('date'),\n",
    "                                                                                lit(' '),\n",
    "                                                                                col('time'),\n",
    "                                                                                ),\n",
    "                                                                            format = 'yyyy-MM-dd HH:mm:ss'\n",
    "                                                                            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b61fa2-4f5e-4fc0-9777-8fd5eded102a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 3 - Select only the columns required & rename as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f94fe91f-649e-40d0-93e1-1f07e6613f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_selected_df = races_with_timestamp_df.select(col('raceId').alias('race_id'),\n",
    "                                                   col('year').alias('race_year'),\n",
    "                                                   col('round'),\n",
    "                                                   col('circuitId').alias('circuit_id'),\n",
    "                                                   col('name'),\n",
    "                                                   col('ingestion_date'),\n",
    "                                                   col('race_timestamp')\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "390bf90b-b8b0-4a58-af00-5a2de77e5a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Write the output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "236e763d-a6f1-4f68-ba85-bf410cb82f29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "races_selected_df.write.mode(\"overwrite\").partitionBy(\"race_year\").format(\"parquet\").saveAsTable(\"f1_processed.races\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228a8c93-7ba4-4094-9fc7-3e783e33bc03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1145852851449792,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.ingest_races_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
