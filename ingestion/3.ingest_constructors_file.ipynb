{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e5f24f-d383-4146-8f51-3008ee992b77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ingest constructor.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6c74db-197e-4a8c-8a9d-c566f50b0c50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 1 - Read the JSON file using the spark dataframe reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8e35c7-abd4-402a-af28-b15456329832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_schema = \"constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd3a21c-9340-4dd5-85ff-b85c52d4e44e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_df = spark.read.format(\"json\")\\\n",
    "                           .schema(schema=constructor_schema)\\\n",
    "                           .load(\"/mnt/tideformula1dl/raw/constructors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3bf9753-6531-48e3-bec0-2c92a5b25421",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 2 - Drop unwanted columns from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5436d221-1999-4717-83ee-b51e93c2af43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79cd9d9e-c600-4f3d-ba18-317150b5e1aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_dropped_df = constructor_df.drop(col('url'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d70c77-74ba-44fb-8d8e-2c5fa3e5208e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 3 - Rename columns and add ingestion date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5895dc-5c23-409f-bb8b-9033a424efe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "constructor_final_df = constructor_dropped_df.withColumnRenamed(\"constructorId\",\"constructor_id\")\\\n",
    "                                             .withColumnRenamed(\"constructorRef\",\"constructor_ref\")\\\n",
    "                                             .withColumn(\"ingestion_date\",current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8b96d92-afee-4315-b2c5-809fcdd2a474",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 4 - Write output to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0762dcdc-589b-4910-84b3-f783b0c3d01a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "constructor_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.constructors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73578d0b-2187-41c3-b084-a6a3e0643f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.ingest_constructors_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
