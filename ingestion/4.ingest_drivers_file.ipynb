{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f2f8fc-a9f2-4c64-b118-16f6d84d4b7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ingest drivers.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e35f4fc-25b9-4520-bddb-bd31f065325c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab00af55-6bf1-4b60-b409-9cc13eb22070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074aefca-f8ed-48fb-9a5a-0630f812d6ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name_schema = StructType(fields=[\n",
    "    StructField(\"forename\", StringType(), nullable=True),\n",
    "    StructField(\"surname\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13effd8d-c333-458e-884b-8716bce944cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_schema = StructType(fields=[\n",
    "    StructField(\"driverId\", IntegerType(), nullable=False),\n",
    "    StructField(\"driverRef\", StringType(), nullable=True),\n",
    "    StructField(\"number\", IntegerType(), nullable=True),\n",
    "    StructField(\"code\", StringType(), nullable=True),\n",
    "    StructField(\"name\", name_schema),\n",
    "    StructField(\"dob\", DateType(), nullable=True),\n",
    "    StructField(\"nationality\", StringType(), nullable=True),\n",
    "    StructField(\"url\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35ce5ec-35d1-4254-85be-6daad98417de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_df = spark.read.format(\"json\")\\\n",
    "                       .schema(drivers_schema)\\\n",
    "                       .load(\"/mnt/tideformula1dl/raw/drivers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52309220-cd7c-453c-bd0d-0c447aec78ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 2 - Rename columns and add new comlumns\n",
    "1. driverId renamed to driver_id\n",
    "2. driverRef renamed to driver_ref\n",
    "3. ingestion date added\n",
    "4. name added with concatenation of forename and surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c99f85d-3127-4e89-935a-5198b8ccdb1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, current_timestamp, lit\n",
    "\n",
    "drivers_with_columns_df = drivers_df.withColumnRenamed(\"driverId\", \"driver_id\")\\\n",
    "                                    .withColumnRenamed(\"driverRef\", \"driver_ref\")\\\n",
    "                                    .withColumn(\"ingestion_date\", current_timestamp())\\\n",
    "                                    .withColumn(\"name\", concat(col(\"name.forename\"), lit(\" \"), col(\"name.surname\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d93f27b-3d2e-414d-806d-ef4b1092d4b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 3 - Drop the unwanted columns\n",
    "1. name.forename\n",
    "2. name.surname\n",
    "3. url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07068e4-5530-4cb1-af0c-439f931826c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_final_df = drivers_with_columns_df.drop(col(\"url\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8eca26-b77d-4eb5-857c-6bd8bcbea034",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 4 - Write to the ouput to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519c0bd2-7759-4135-9fa7-7a7dd1d71497",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.drivers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac838586-98db-46e1-abf9-5da9c4399eea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.ingest_drivers_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
