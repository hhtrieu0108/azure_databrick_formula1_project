{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c367770a-19da-4283-9c79-3753f1e98282",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ingest lap_times folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce47f4cd-a3e1-44cb-b28c-c45106f2c757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Step 1 - Read the CSV file using the spark dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac832a9-c116-4ab4-939c-b841c886d41e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f794f5f-3341-4619-b857-0c62d4207b6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lap_times_schema = StructType(fields=[\n",
    "    StructField(\"raceId\", IntegerType(), nullable=False),\n",
    "    StructField(\"driverId\", IntegerType(), nullable=True),\n",
    "    StructField(\"lap\", IntegerType(), nullable=True),\n",
    "    StructField(\"position\", IntegerType(), nullable=True),\n",
    "    StructField(\"time\", StringType(), nullable=True),\n",
    "    StructField(\"milliseconds\", IntegerType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "956d9081-9fae-40fb-86b9-7c79316bdf10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lap_times_df = spark.read.format(\"csv\")\\\n",
    "                         .schema(lap_times_schema)\\\n",
    "                         .load(\"/mnt/tideformula1dl/raw/lap_times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e604da-ae5d-47ec-8d4b-fec21b734a08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 2 - Rename columns and add new columns\n",
    "1. Rename driverId and raceId\n",
    "2. Add ingestion_date with current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030c3ef0-a40a-495f-920b-43034e26c058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9df5ae6-2c00-4df9-916f-df0cbda9b352",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = lap_times_df.withColumnRenamed(\"driverId\", \"driver_id\")\\\n",
    "                      .withColumnRenamed(\"raceId\", \"race_id\")\\\n",
    "                      .withColumn(\"ingestion_date\",current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b517b57d-4914-4b38-85ac-fece73383027",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 3 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9aa7b16-ad1f-4e10-ac90-8de8397d2a69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.lap_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ee623a-d2ae-4d4a-af59-6c24e473fb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aae42425-b5d3-44d6-b169-f4392fdb7a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888f7d7c-d06f-459c-a131-91c2ab3319a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7.ingest_lap_times_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
