{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00dd9201-f41d-4cde-ab0a-b6596f4f8da8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Ingest result file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba14c3f2-a10e-4206-83de-bfb6bcd6ad56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb67623a-abaa-41a5-ba10-bc559f80b46e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d390642c-f828-4ccc-8d3e-643cae59b435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_schema = StructType(fields=[\n",
    "    StructField(\"resultId\", IntegerType(), nullable=False),\n",
    "    StructField(\"raceId\", IntegerType(), nullable=True),\n",
    "    StructField(\"driverId\", IntegerType(), nullable=True),\n",
    "    StructField(\"constructorId\", IntegerType(), nullable=True),\n",
    "    StructField(\"number\", IntegerType(), nullable=True),\n",
    "    StructField(\"grid\", IntegerType(), nullable=True),\n",
    "    StructField(\"position\", IntegerType(), nullable=True),\n",
    "    StructField(\"positionText\", StringType(), nullable=True),\n",
    "    StructField(\"positionOrder\", IntegerType(), nullable=True),\n",
    "    StructField(\"points\", FloatType(), nullable=True),\n",
    "    StructField(\"laps\", IntegerType(), nullable=True),\n",
    "    StructField(\"time\", StringType(), nullable=True),\n",
    "    StructField(\"milliseconds\", IntegerType(), nullable=True),\n",
    "    StructField(\"fastestLap\", IntegerType(), nullable=True),\n",
    "    StructField(\"rank\", IntegerType(), nullable=True),\n",
    "    StructField(\"fastestLapTime\", StringType(), nullable=True),\n",
    "    StructField(\"fastestLapSpeed\", FloatType(), nullable=True),\n",
    "    StructField(\"statusId\", StringType(), nullable=True)   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180039c0-c2e6-4d6e-9d84-76cda212f6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = spark.read.format(\"json\")\\\n",
    "                       .schema(results_schema)\\\n",
    "                       .load(\"/mnt/tideformula1dl/raw/results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a05b59c5-08b3-47f8-85b6-ae70d208a77b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 2 - Rename columns and add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1319dcbd-49a8-4120-a5cf-a003dbf47c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "results_with_columns_df = results_df.withColumnRenamed(\"resultId\",\"result_id\") \\\n",
    "                                    .withColumnRenamed(\"raceId\",\"race_id\") \\\n",
    "                                    .withColumnRenamed(\"driverId\",\"driver_id\") \\\n",
    "                                    .withColumnRenamed(\"constructorId\",\"constructor_id\") \\\n",
    "                                    .withColumnRenamed(\"positionText\",\"position_text\") \\\n",
    "                                    .withColumnRenamed(\"positionOrder\",\"position_order\") \\\n",
    "                                    .withColumnRenamed(\"fastestLap\",\"fastest_lap\") \\\n",
    "                                    .withColumnRenamed(\"fastestLapTime\",\"fastest_lap_time\") \\\n",
    "                                    .withColumnRenamed(\"fastestLapSpeed\",\"fastest_lap_speed\") \\\n",
    "                                    .withColumn(\"ingestion_date\",current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b4ad731-ba45-4c35-b9fc-76a3caa4fd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 3 - Drop the unwanted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ac5f38-1aa6-4002-8d26-57af804ad99a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "results_final_df = results_with_columns_df.drop(col(\"statusId\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b490812e-420d-4fea-91d8-5d5865961b89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Step 4 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5193d038-b5a8-492e-af2c-9c19cc2b1039",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_final_df.write.mode(\"overwrite\").partitionBy(\"race_id\").format(\"parquet\").saveAsTable(\"f1_processed.results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77fb8e9e-ca9e-4093-a229-d27dbecdebf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.ingest_result_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
